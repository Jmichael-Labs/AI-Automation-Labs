#!/usr/bin/env python3
"""
End-to-End Testing Suite for Multi-Platform AI Education System
Tests the complete 16-channel automation pipeline
"""

import os
import sys
import time
import json
from datetime import datetime
import subprocess

def test_dependencies():
    """Test all required Python dependencies"""
    print("üì¶ Testing Python dependencies...")
    
    required_packages = [
        'requests',
        'google-cloud-language',
        'praw'  # For Reddit integration
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package.replace('-', '_'))
            print(f"‚úÖ {package}: Available")
        except ImportError:
            print(f"‚ùå {package}: Missing")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\nüîß Install missing packages: pip install {' '.join(missing_packages)}")
        return False
    
    return True

def test_environment_variables():
    """Test if all required environment variables are set"""
    print("\nüîß Testing environment variables...")
    
    required_vars = [
        # Telegram tokens
        'TELEGRAM_LEGAL_TOKEN', 'TELEGRAM_MEDICAL_TOKEN', 
        'TELEGRAM_SENIOR_TOKEN', 'TELEGRAM_GENERAL_TOKEN',
        # Ko-fi webhooks
        'KOFI_LEGAL_API', 'KOFI_MEDICAL_API', 
        'KOFI_SENIOR_API', 'KOFI_GENERAL_API',
        # ConvertKit
        'CONVERTKIT_API_KEY', 'CONVERTKIT_LEGAL_FORM', 
        'CONVERTKIT_MEDICAL_FORM', 'CONVERTKIT_SENIOR_FORM', 'CONVERTKIT_GENERAL_FORM',
        # Gumroad
        'GUMROAD_API_KEY', 'GUMROAD_LEGAL_ID', 
        'GUMROAD_MEDICAL_ID', 'GUMROAD_SENIOR_ID', 'GUMROAD_GENERAL_ID',
        # Google Cloud
        'GOOGLE_CLOUD_CREDENTIALS', 'GOOGLE_PROJECT_ID',
        # General
        'EMAIL_CONTACT', 'INSTAGRAM_CONSULTING'
    ]
    
    missing_vars = []
    for var in required_vars:
        if os.getenv(var):
            print(f"‚úÖ {var}: Set")
        else:
            print(f"‚ùå {var}: Missing")
            missing_vars.append(var)
    
    if missing_vars:
        print(f"\nüö® {len(missing_vars)} environment variables missing!")
        print("üìã Run: export VARIABLE_NAME='your_value' for each missing variable")
        return False
    
    return True

def test_infinite_content_engine():
    """Test the infinite content engine"""
    print("\nüé≠ Testing Infinite Content Engine...")
    
    try:
        from infinite_content_engine import InfiniteContentEngine
        
        engine = InfiniteContentEngine()
        content = engine.generate_infinite_content()
        
        if content:
            title, body = content
            print(f"‚úÖ Content generated: '{title[:50]}...'")
            print(f"üìù Content length: {len(body)} characters")
            return True
        else:
            print("‚ùå Content generation failed")
            return False
            
    except Exception as e:
        print(f"‚ùå Content engine error: {e}")
        return False

def test_multi_platform_engine():
    """Test the multi-platform publishing engine"""
    print("\nüöÄ Testing Multi-Platform Engine...")
    
    try:
        from multi_platform_engine import MultiPlatformEngine
        
        engine = MultiPlatformEngine()
        print("‚úÖ Multi-Platform Engine initialized")
        
        # Test content classification
        test_content = "This AI tool helps lawyers automate legal document review and contract analysis."
        industry, confidence = engine.classify_content_industry(test_content)
        print(f"‚úÖ Content classified as: {industry} (confidence: {confidence})")
        
        # Test content adaptation
        adapted = engine.adapt_content_for_industry(test_content, industry, "telegram")
        print(f"‚úÖ Content adapted for {industry}/telegram: {len(adapted)} chars")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Multi-Platform Engine error: {e}")
        return False

def test_api_integrations():
    """Test API integrations with mock data"""
    print("\nüîå Testing API integrations...")
    
    try:
        from multi_platform_engine import MultiPlatformEngine
        
        engine = MultiPlatformEngine()
        test_content = "üß† AI Education Test: This is a test message for API validation."
        
        # Test each platform with legal industry
        test_industry = "legal"
        
        print(f"üì° Testing {test_industry} industry APIs...")
        
        # Test Telegram (should work if token is valid)
        try:
            result = engine.publish_to_telegram(test_industry, test_content)
            print(f"‚úÖ Telegram {test_industry}: {'Success' if result else 'Failed'}")
        except Exception as e:
            print(f"‚ö†Ô∏è Telegram {test_industry}: {e}")
        
        # Test other platforms (will validate credentials and endpoints)
        platforms = ["kofi", "convertkit", "gumroad"]
        for platform in platforms:
            try:
                if platform == "kofi":
                    result = engine.publish_to_kofi(test_industry, test_content)
                elif platform == "convertkit":
                    result = engine.publish_to_convertkit(test_industry, test_content)
                elif platform == "gumroad":
                    result = engine.publish_to_gumroad(test_industry, test_content)
                
                print(f"‚úÖ {platform.title()} {test_industry}: {'Success' if result else 'Failed'}")
            except Exception as e:
                print(f"‚ö†Ô∏è {platform.title()} {test_industry}: {e}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå API integration test error: {e}")
        return False

def test_github_actions_workflow():
    """Test GitHub Actions workflow structure"""
    print("\n‚öôÔ∏è Testing GitHub Actions workflow...")
    
    workflow_path = "/Volumes/DiskExFAT 1/reddit_ai_bot_production/.github/workflows/multi_platform_publishing.yml"
    
    if os.path.exists(workflow_path):
        print("‚úÖ GitHub Actions workflow file exists")
        
        try:
            with open(workflow_path, 'r') as f:
                workflow_content = f.read()
            
            # Check for required sections
            required_sections = [
                'Multi-Platform AI Education Publishing System',
                'TELEGRAM_LEGAL_TOKEN',
                'CONVERTKIT_API_KEY',
                'GUMROAD_API_KEY',
                'multi_platform_engine.py'
            ]
            
            all_sections_found = True
            for section in required_sections:
                if section in workflow_content:
                    print(f"‚úÖ Workflow contains: {section}")
                else:
                    print(f"‚ùå Workflow missing: {section}")
                    all_sections_found = False
            
            return all_sections_found
            
        except Exception as e:
            print(f"‚ùå Error reading workflow: {e}")
            return False
    else:
        print("‚ùå GitHub Actions workflow file not found")
        return False

def test_credentials_validator():
    """Test the credentials validator"""
    print("\nüîê Testing credentials validator...")
    
    try:
        # Run the credential validator
        result = subprocess.run(
            [sys.executable, "validate_credentials.py"],
            cwd="/Volumes/DiskExFAT 1/reddit_ai_bot_production",
            capture_output=True,
            text=True,
            timeout=60
        )
        
        if result.returncode == 0:
            print("‚úÖ Credential validation passed")
            return True
        else:
            print("‚ö†Ô∏è Some credential validations failed")
            print("üìã Run 'python validate_credentials.py' for details")
            return False
            
    except subprocess.TimeoutExpired:
        print("‚ö†Ô∏è Credential validation timed out (may indicate API issues)")
        return False
    except Exception as e:
        print(f"‚ùå Credential validation error: {e}")
        return False

def simulate_full_publishing_cycle():
    """Simulate a complete publishing cycle"""
    print("\nüéØ Simulating full publishing cycle...")
    
    try:
        from multi_platform_engine import MultiPlatformEngine
        
        engine = MultiPlatformEngine()
        
        # Generate content
        print("üìù Generating content...")
        publications = engine.run_daily_publishing_cycle()
        
        if publications > 0:
            print(f"‚úÖ Publishing cycle completed: {publications} publications")
            return True
        else:
            print("‚ö†Ô∏è Publishing cycle completed but no publications succeeded")
            return False
            
    except Exception as e:
        print(f"‚ùå Publishing cycle simulation error: {e}")
        return False

def calculate_system_readiness():
    """Calculate overall system readiness percentage"""
    print("\n" + "="*60)
    print("üèÅ SYSTEM READINESS ASSESSMENT")
    print("="*60)
    
    tests = [
        ("Dependencies", test_dependencies),
        ("Environment Variables", test_environment_variables),
        ("Content Engine", test_infinite_content_engine),
        ("Multi-Platform Engine", test_multi_platform_engine),
        ("API Integrations", test_api_integrations),
        ("GitHub Actions", test_github_actions_workflow),
        ("Credential Validation", test_credentials_validator),
        ("Full Cycle Simulation", simulate_full_publishing_cycle)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"‚ùå {test_name} failed with exception: {e}")
            results[test_name] = False
    
    # Calculate readiness
    total_tests = len(results)
    passed_tests = sum(1 for result in results.values() if result)
    readiness_percentage = (passed_tests / total_tests) * 100
    
    print("\n" + "="*60)
    print("üìä FINAL RESULTS")
    print("="*60)
    
    for test_name, result in results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{test_name:25} {status}")
    
    print(f"\nüéØ System Readiness: {readiness_percentage:.1f}% ({passed_tests}/{total_tests} tests passed)")
    
    if readiness_percentage >= 90:
        print("üöÄ EXCELLENT: System ready for production deployment!")
    elif readiness_percentage >= 70:
        print("‚úÖ GOOD: System mostly ready, minor fixes needed")
    elif readiness_percentage >= 50:
        print("‚ö†Ô∏è MODERATE: System needs significant configuration")
    else:
        print("‚ùå CRITICAL: System requires major fixes before deployment")
    
    # Deployment recommendation
    print("\nüîß NEXT STEPS:")
    if readiness_percentage >= 80:
        print("1. ‚úÖ Configure remaining credentials if any")
        print("2. ‚úÖ Test with GitHub Actions manual trigger")
        print("3. ‚úÖ Monitor automated publishing (3x daily)")
        print("4. ‚úÖ Scale to additional industries/platforms")
    else:
        failed_tests = [name for name, result in results.items() if not result]
        print("1. üîß Fix failed components:")
        for failed_test in failed_tests:
            print(f"   - {failed_test}")
        print("2. üîß Re-run end-to-end testing")
        print("3. üîß Verify credentials setup guide")
    
    return readiness_percentage

def main():
    """Run complete end-to-end testing suite"""
    print("üß™ MULTI-PLATFORM AI EDUCATION SYSTEM")
    print("üß™ END-TO-END TESTING SUITE")
    print("="*60)
    print(f"üìÖ Test execution: {datetime.now()}")
    print(f"üìÇ Working directory: {os.getcwd()}")
    print()
    
    readiness = calculate_system_readiness()
    
    print(f"\n‚è±Ô∏è Testing completed at: {datetime.now()}")
    
    # Exit with appropriate code
    if readiness >= 80:
        exit(0)  # Success
    else:
        exit(1)  # Failure

if __name__ == "__main__":
    main()